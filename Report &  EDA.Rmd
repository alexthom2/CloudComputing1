---
title: "Report"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


# Introduction 

This project looks at a sample of data form the terapixel images cloude system for the newcastle urban oberservatory. The system is set up to be scalable depening on user demands and outpuits a 3d visualisation of newcastle city scentre. This project looks at the performance of the virtual machines and looks at developing a set of visualisations that can be regularly run to understand the current perofrmance of the system. Currently there is no set regime of perofmrance monitoring of the cloud based uystem and in order to improve user experiences by ensuring the system is fast and responsive for every user a set of visualisations will be developed. The plan is to review the run times of the 1024 GPU nodes and identify nodes that have below average performance. 


# Methods 

The first task was to conduct an explortatory analysis of the data. The 3 data samples were read into R and first there structure and composition was reviewed. Columns which could be used to join data where identified. It was njoticed that the run times were listen so the data was split in order to calculate the run times of the stages. Once that was complete an unerstanding of the full run time for a render was calculated and a histogram plotted. Once this was complete and the difference in run times was observed furth clculation was done to see the times taken for the inividual tasks. Using this data the shiny application was created. Origionlly the plan was to be able to select any of the 1024 nodes however, this was too many for the selection so in the end the app is limited to the top 10 slowest nodes. On the app once the node is selected there is a summary of the time taken for each part of the render compared to other nodes so and understanding of whats delayed that particular node. Also a summary of the last few hours total render times for that node so an appreication of the general time taken for that node and it can be seen if the slow performance is consistent or if it was an isolated run of the virtual machine. 


# Results 
 
Overall i think this project is a good starting point to further refine the output dashboard to identify more critical performance metrics. It is clear when slecting a number of different nodes that most of the time delay rendering a pixel are caused by uploading the data. This is a clear strength of the output that its clear to see this impact on the performance and time taken to render. One area that could be improved would be using the GPU temperature and power utilisation data. This could then give further insights such as identifying a node which is running hot and therefore impacting its performance. This could then be a key performance monitoring signal. Implications for the future are that this could be used, with some further development, as a live monitoring tool for the virtual machine. This was a limited dataset covering roughly an hour of the systems run time however this shiny dahboard oculd be linked up live to the system. This could then provide real time and hisotircal performance monitoring of the system as a simple date filter could be added which would allow you to go to a particular day and time. 


# Reflection 

The process was initially chalenging understanding the data and being able to know what the link between the 3 datsets are. Also the management of dates and times in order to calculate acurate durations. Once I understood the datasets ebing able to munge the data and create the dataframes needed for plottoing i found relatively simple. I feel pretty comfortable in general manipulating data and this data allowed me to stretch my abilities in that area and further gain confidence in undertaking it. It was good to strectch myself and create a shiny dashboard which could form the basis of a usable perofrmance monitoring tool. Understanding the structure of a how a shiny dahboard is built and what the fundemental building blocks of a shiny dahboard is is a great starting point to using the methodolgies. Althogh the dashboard itself is realtively simple being able to create a simple dashboard in shiny is great starting point to further developing my shiny dashboard skills. 



```{r}


## packages used 
library(tidyverse)

library(lubridate)

```



```{r}

# data read from the 
setwd("~/Data Science Masters/cloudcomputing/data")

gpu <- read_csv("gpu.csv")

appcheck <- read_csv("application-checkpoints.csv")

task <- read_csv("task-x-y.csv")



```

```{r}


# reviewing the structure of the 3 data frames 

str(gpu)



str(appcheck)


str(task)






```




The Data is made up of 3 data frames. Ther first the appacheck which is the action of the virtual machine at that precise time. GPU which is static data for the GPU and task identifies which precise pixel the virtual machine was rendering at the time of the task. The first thing to look at is how the individual takss are broken down 


```{r}





appcheck %>% group_by(jobId) %>%
                              summarise(n = n())




 appcheck %>% group_by(taskId) %>%
                              summarise(n = n())



appcheck %>% group_by(hostname) %>%
                              summarise(n = n())


```


Jobid seems to not be too informative with only 3 levels. Taskid looks to be related to the generation of a pixel and looks to be the area that will be investigated. hostname is each GPU which we will be monitoring the performance of. 






```{r}


host_us <- appcheck %>%  filter(eventName == "TotalRender")  %>%
                            arrange(timestamp) %>%
                             group_by(hostname, taskId) %>%
                            mutate(rown = 1:n()) %>%
                              filter(rown == 1) 



host_us_end <- appcheck %>%  filter(eventName == "TotalRender") %>%
                                 arrange(timestamp) %>%
                            group_by(hostname, taskId) %>%
                               mutate(rown = 1:n()) %>%
                              slice_max(rown) %>%
                              select(timestamp, hostname, taskId)


colnames(host_us_end)[1] <- "timeend"


data <- host_us %>% left_join(host_us_end, by = c("hostname", "taskId")) %>%
                            mutate(duration = timeend - timestamp)
                             


ggplot(data, aes(x = duration)) + geom_histogram()







```

There is a range of task runtimes from as low as 23 seconds to as high as 93 seconds with most full tasks taking around 40 seconds. This is the area to investigate further to understand the cause of the difference 





```{r}


host_us <- appcheck %>%  filter(eventName == "Saving Config")  %>%
                            arrange(timestamp) %>%
                             group_by(hostname, taskId) %>%
                            mutate(rown = 1:n()) %>%
                              filter(rown == 1) 
                          



host_us_end <- appcheck %>%  filter(eventName == "Saving Config") %>%
                                 arrange(timestamp) %>%
                            group_by(hostname, taskId) %>%
                               mutate(rown = 1:n()) %>%
                              slice_max(rown) %>%
                              select(timestamp, hostname, taskId) 


colnames(host_us_end)[1] <- "timeend"


data_conf <- host_us %>% left_join(host_us_end, by = c("hostname", "taskId")) %>%
                            mutate(duration = timeend - timestamp) %>%
                            select(timestamp, hostname, taskId, duration)

colnames(data_conf)[4] <- "Sconf_t"


host_us <- appcheck %>%  filter(eventName == "Tiling")  %>%
                            arrange(timestamp) %>%
                             group_by(hostname, taskId) %>%
                            mutate(rown = 1:n()) %>%
                              filter(rown == 1) 
                          



host_us_end <- appcheck %>%  filter(eventName == "Tiling") %>%
                                 arrange(timestamp) %>%
                            group_by(hostname, taskId) %>%
                               mutate(rown = 1:n()) %>%
                              slice_max(rown) %>%
                              select(timestamp, hostname, taskId)


colnames(host_us_end)[1] <- "timeend"


data_tiling <- host_us %>% left_join(host_us_end, by = c("hostname", "taskId")) %>%
                            mutate(duration = timeend - timestamp) %>%
                            select(timestamp, hostname, taskId, duration)

colnames(data_tiling)[4] <- "tiling_t"


host_us <- appcheck %>%  filter(eventName == "Render")  %>%
                            arrange(timestamp) %>%
                             group_by(hostname, taskId) %>%
                            mutate(rown = 1:n()) %>%
                              filter(rown == 1) 
                          



host_us_end <- appcheck %>%  filter(eventName == "Render") %>%
                                 arrange(timestamp) %>%
                            group_by(hostname, taskId) %>%
                               mutate(rown = 1:n()) %>%
                              slice_max(rown) %>%
                              select(timestamp, hostname, taskId)


colnames(host_us_end)[1] <- "timeend"


data_rend <- host_us %>% left_join(host_us_end, by = c("hostname", "taskId")) %>%
                            mutate(duration = timeend - timestamp)  %>%
                            select(timestamp, hostname, taskId, duration)

colnames(data_rend)[4] <- "render_t"

host_us <- appcheck %>%  filter(eventName == "Uploading")  %>%
                            arrange(timestamp) %>%
                             group_by(hostname, taskId) %>%
                            mutate(rown = 1:n()) %>%
                              filter(rown == 1) 
                          



host_us_end <- appcheck %>%  filter(eventName == "Uploading") %>%
                                 arrange(timestamp) %>%
                            group_by(hostname, taskId) %>%
                               mutate(rown = 1:n()) %>%
                              slice_max(rown) %>%
                              select(timestamp, hostname, taskId)


colnames(host_us_end)[1] <- "timeend"


data_up <- host_us %>% left_join(host_us_end, by = c("hostname", "taskId")) %>%
                            mutate(duration = timeend - timestamp)  %>%
                            select(timestamp, hostname, taskId, duration)


colnames(data_up)[4] <- "upload_t"


full_data <- data %>% left_join(data_conf, by = c( "hostname", "taskId")) %>% 
                          left_join(data_rend, by = c( "hostname", "taskId")) %>% 
                              left_join(data_tiling, by = c( "hostname", "taskId")) %>% 
                                left_join(data_up, by = c( "hostname", "taskId")) %>% 
                              left_join(task, by = "taskId")  
                                


```



```{r}


ggplot(full_data, aes(x = upload_t)) + geom_histogram()



ggplot(full_data, aes(x = render_t)) + geom_histogram()




ggplot(full_data, aes(x = Sconf_t)) + geom_histogram()





ggplot(full_data, aes(x = tiling_t)) + geom_histogram()





```




```{r}


ggplot(full_data,  aes(x = x, y = y, col = duration)) + geom_point()


```



```{r}


shiny_dat <-  data %>% left_join(data_conf, by = c( "hostname", "taskId")) %>% 
                          left_join(data_rend, by = c( "hostname", "taskId")) %>% 
                              left_join(data_tiling, by = c( "hostname", "taskId")) %>% 
                                left_join(data_up, by = c( "hostname", "taskId")) %>% 
                              left_join(task, by = "taskId") %>% 
                                select(timestamp.x, hostname, taskId, duration, Sconf_t, render_t, tiling_t, upload_t) %>% 
                                      pivot_longer(cols = 4:8, names_to = "stage", values_to = "time")


ggplot(shiny_dat, aes(x = stage, y= time)) + geom_jitter(width = 0.1) + facet_wrap(stage~., scales = "free") + coord_flip()



```



```{r}


host_av <- full_data %>% group_by(hostname) %>% 
                            summarise(meandur = mean(duration)) %>% 
                              mutate(rank = rank(desc(meandur))) 

full_data2 <- full_data %>% left_join(host_av, by = "hostname") %>% 
                                   select(timestamp.x, hostname, taskId, duration, Sconf_t, render_t, tiling_t, upload_t) %>% 
  pivot_longer(cols = 4:8, names_to = "stage", values_to = "time") 
                                  
```


```{r}



fulldata3 <- full_data2 %>% filter(stage == "duration") %>% 
                              filter(hostname == "db871cd77a544e13bc791a64a0c8ed5000000U")


ggplot(fulldata3, aes(x = timestamp.x, y = time)) + geom_point()

```

